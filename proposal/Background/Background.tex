This chapter provides details necessary to understand the investigations and experiments in this thesis.
I focus on storage technologies, database analytics optimization, database transaction processing optimizations, and memory consistency models.
The purpose of discussing database optimizations is to understand the complications that arise when using disk, and how resulting mechanisms interact with new storage technologies.
Additionally, memory consistency forms the foundation for memory persistency.

\section{Storage Technologies}
\label{sec:Background:Storage}

Many technologies provide storage persistent storage, including disk, Flash, and upcoming NVRAM.
For each, I outline the operating principles and important technological trends.

\input{Background/tab.devices.tex}

\subsection{Disk}
\label{sec:Background:Storage:Disk}
Disk has been the primary durable and high capacity storage technology for decades.
Disk functions by storing data on spinning magnetic platters.
Accessing data requires moving the \emph{hard disk head} onto the proper \emph{track}.
Once the head settles, it may read or write data as the platter rotates and the correct sector within the track reaches the head.
Disk capacity increases with the areal size and number of platters, as well as areal density (placing more sectors and tracks within the same area).
Because capacity has scaled so well (and continues to), disk remains the lowest cost technology for large datasets and persistent storage.

While popular and inexpensive, disk suffers relatively slow access and undesirable access behavior \cite{RuemmlerWilkes94}.
Rotational speed limits the rate that data transfers to or from the disk.
Further, random reads and writes must first seek to the proper track and then wait until the correct sector reaches the head, a process which takes several ms.
Table~\ref{table:DiskCharacteristics} lists the measured performance characteristics of an enterprise disk (2011).
This disk achieves nearly 120 MB/s sequential transfers, but random reads take an average of 10ms (only 50 KB/s for 512-byte sectors).
As a result, there is a large history of optimization for disk-resident storage, as I discuss in Section~\ref{sec:Background:Recovery}.

\subsection{Flash Memory}
\label{sec:Background:Storage:Flash}
Driven by the popularity of mobile devices, Flash memory has quickly improved in both storage density and cost to the point where it has become a viable alternative for durable storage in enterprise systems.
Unlike conventional rotating hard disks, which store data using magnetic materials, Flash stores charge on a floating-gate transistor forming a memory cell.
These transistors are arranged in arrays resembling NAND logic gates, after which the ``NAND Flash'' technology is named.
This layout gives NAND Flash a high storage density relative to other memory technologies.
Though dense, the layout allows only coarse-grained access and sacrifices read latency---an entire array (a.k.a. page, typically 2KB to 4KB) must be read in a single operation---making NAND Flash more appropriate for block-oriented IO than as a direct replacement for RAM.  

One of the difficulties of Flash devices is that a cell can be more easily programmed (by adding electrons to the floating gate) than erased (removing these electrons).  
Erase operations require both greater energy and latency, and typically can be applied only at coarse granularity (e.g., over blocks of 128KB to 512KB).
Moreover, repeated erase operations cause the Flash cell to wear out over time, limiting cell lifetime (e.g., to $10^5$ to $10^6$ writes \cite{Roberts2009}).  
Recent Flash devices further increase storage density by using several distinct charge values to represent multiple bits in a single cell at the cost of slower accesses and even shorter lifetimes.

A Flash-based SSD wraps an array of underlying Flash memory chips with a controller that manages capacity allocation, mapping, and wear leveling across the individual Flash devices.  
The controller mimics the interface of a conventional (e.g., SATA) hard drive, allowing Flash SSDs to be drop-in replacements for conventional disks.
      
As previously noted, Flash SSDs provide substantially better performance than disks, particularly for random reads, but at higher cost \cite{BoboilaDesoyers11}.
Table \ref{table:DiskCharacteristics} lists specifications of a typical Flash SSD as compared to a 10,000 RPM hard drive (2011).
Though neither of these devices are the highest-performing available today, they are representative of the mid-range of their respective markets.
The latency for a random read is over 100\texttimes~better on the SSD than on the disk, while the sequential read bandwidth is 1.6\texttimes~better. 
Unlike disks, where each random read incurs mechanical delays (disk head seek and rotational delays), on SSDs, a random read is nearly as fast as a sequential read.  

\subsection{NVRAM}
\label{sec:Background:Storage:NVRAM}

Nonvolatile memories will soon be commonplace.
Technology trends suggest that DRAM and Flash memory may cease to scale, requiring new dense memory technologies \cite{LeeIpek09}.

\textbf{Memory technology characteristics.}
Numerous technologies offer durable byte-addressable access.
Examples include phase change memory (PCM), where a chalcogenide glass is heated to produce varying electrical conductivities, and spin-transfer torque memory (STT-RAM), a magnetic memory that stores state in electron spin \cite{BurrKurdi08}.
Storage capacity increases by storing more than two states per cell in Multi-level Cells (MLC) (e.g., four distinct resistivity levels provide storage of 2 bits per cell).

While it remains unclear which of these technologies will eventually gain traction, many share common characteristics.
In particular, NVRAMs will likely provide somewhat higher access latency relative to DRAM.
Furthermore, several technologies are expected to have asymmetric read-write latencies, where writing to the device may take several microseconds \cite{QureshiSrinivasan09}.
Write latency worsens with MLC, where slow, iterative writes are necessary to reliably write to cells.

Similarly to Flash, resistive NVRAM technologies suffer from limited write endurance; cells may be written reliably a limited number of times.
While write endurance is an important consideration, proposed hardware mechanisms (e.g., Start-Gap \cite{QureshiKaridis09}) are effective in distributing writes across cells, mitigating write endurance concerns.

\textbf{NVRAM storage architectures.}
Future database systems may incorporate NVRAM in a variety of ways.
At one extreme, NVRAM can be deployed as a disk or Flash SSD replacement.
While safe, cost-effective, and backwards compatible, the traditional disk interface imposes overheads.
Prior work demonstrates that file system and disk controller latencies dominate NVRAM access times \cite{CaulfieldDe10}.
Furthermore, block access negates advantages of fine-grained access.

Recent research proposes alternative interfaces for NVRAM.
Caulfield \emph{et al.} propose Moneta and Moneta Direct, a PCIe attached PCM device \cite{CaulfieldMollov12}.
Unlike disk, Moneta Direct bypasses expensive system software and disk controller hardware to minimize access latency while still providing traditional file system semantics.
However, Moneta retains a block interface.
Condit \emph{et al.} suggest that NVRAM connect directly to the memory bus, with additional hardware and software mechanisms providing file system access and consistency \cite{ConditNightingale09}.
I later adopt the same atomic eight-byte persistent write, enabling small, safe writes even in the face of failure.
NVRAM will eventually connect via a memory interface, but it is unclear how storage technologies will evolve or what their exact performance characteristics will be.
Instead of assuming specific NVRAM technologies and interconnects, I measure the affect that NVRAM performance (both latency and bandwidth) has on workload throughput.

\section{Analytics Optimization}
\label{sec:Background:Analytics}

Large scale data processing requires efficient use of storage devices.
Relational data is stored in tables (relations).
Tables are a collection of rows (tuples), each row containing values for the different columns (attributes) defined on the table.
I discuss two important operations common to the relational model that are expected to be affected by Flash's performance characteristics: scans and joins.

\subsection{Scans}
\label{sec:Background:Scans}

Each data access requires the query optimizer to choose access paths to each table. 
The goal is to select all relevant rows from the table while retrieving the fewest storage pages, frequently with the use of indexes.
Work on access path selection dates back to the late 1970s \cite{Selinger1979}.

There are two classic scan operators implemented by nearly all commercial DBMS systems: relation scan and index scan.
An index is a database data structure that maps column values to rows within a table, supporting fast look-ups by that column.
Indexes may be ordered (as in an in-memory balanced tree or disk-resident B-Tree) to efficiently retrieve all rows satisfying a range query (e.g., an ordered index on ``last name" would accelerate a query asking for all people whose last name is between ``Pelley" and ``Wenisch").
Additionally, indexes may be clustered (the primary table is stored in sorted/hashed order and supports searching) or non-clustered (the index is separate from the primary store and contains references to rows).
A table can only be physically ordered according to one key, limiting the use of clustered indexes.
Database indexes are themselves stored on disk.
Many types of indexes exist, but all provide a more direct way to filter specific data than scanning an entire data set.

When no indexes are available, the only choice is to perform a \emph{relation scan}, where all data pages in the table are read from disk and scanned tuple-by-tuple to select tuples that satisfy the query.
When a relevant index is available, the DBMS may instead choose to perform an \emph{index scan}, where the execution engine traverses the relevant portion of the index and fetches only pages containing selected tuples as needed.
For clustered indexes (i.e., the row itself exists within the index, and all rows are sorted according to the index key), an index scan is nearly always the preferred access path, regardless of the underlying storage device.  
For non-clustered indexes, whether the optimizer should choose a relation scan or index scan depends on the selectivity of the query (expected number of rows that satisfy the query); relation scans have roughly constant cost regardless of selectivity (cost depends on table size), whereas index scan costs grow approximately linearly with selectivity.
When selectivity is low, the index scan provides greater performance because it minimizes the total amount of data that must be transferred from disk.
However, as selectivity increases, the fixed-cost (and simpler) relation scan becomes the faster option.  
Though the relation scan reads the entire table, it can do so using sequential rather than random IO, leveraging the better sequential IO performance of rotating hard disks.
A classic rule of thumb for access path selection is to choose a relation scan once selectivity exceeds ten percent \cite{RamakrishnanAndGehrke}.

Recent databases implement a third, hybrid scan operator, which I call \emph{rowid-sort scan}.
In this scan operator, the unclustered index is scanned to identify relevant tuples.
However, rather than immediately fetching the underlying data pages, the rowid (a unique identifier that signifies the row's page and offset within that page) of each tuple is stored in a temporary table, which is then sorted at the end of the index scan.
Then, the pages identified in the temporary table are fetched in order, and relevant tuples are returned from the page.
The rowid-sort scan has the advantage that each data page will be fetched from disk only once, even if multiple relevant tuples are located on the page. 
Rowid-sort scan is provided by IBM's DB2, although they may refer to it by a different name.
Other databases use different terminology or algorithms to ensure that each heap page is fetched exactly once (for example, PostgreSQL uses a bitmap index, sorting the list of pages with tuples that satisfy the query \cite{PostgresLossyBitMap}).
Rowid-sort scan is the optimal access path for intermediate selectivities.
Section~\ref{sec:FlashOpti:Scans} investigates choosing an optimal access path depending on whether data resides on disk or Flash.

\subsection{Joins}
\label{sec:Background:Joins}

One of the most important aspects of query optimization is choosing appropriate join algorithms.
The development of join algorithms and optimization strategies dates back over 30 years \cite{Selinger1979,Shapiro1986}. 
Most commercial DBMS systems implement variants of at least three join algorithms: nested-loop join, sort-merge join, and hybrid hash join.
At a high level, the nested-loop join iterates over the inner relation for each tuple of the outer relation; the sort-merge join sorts both relations and then performs concurrent scans of the sorted results; and the hybrid hash join forms in-memory hash tables of partitions of the inner relation and then probes these with tuples from the outer relation.

The relative performance of these algorithms depends on a complex interplay of memory capacity, relation sizes, and the relative costs of random and sequential IOs.  
One example performance model that captures this interplay was proposed by Haas and co-authors \cite{DBLP:journals/vldb/HaasCLS97}.
Their model estimates the number of disk seeks and the size of each data transfer and weights each by a cost based on assumed characteristics of the IO device.
The model further identifies the optimal buffering strategy for the various phases of each join algorithm.
Seek and random/sequential transfer times are central parameters of this model, suggesting that new technologies require new device-specific query optimization.

As accessing large amounts of data on disk can limit system throughput, it is imperative that the query optimizer choose the best query plan.
Typical query optimizers use data statistics to approximate query selectivity and cardinality as well as physically-based models to estimate the runtime of candidate query plans.
Sections~\ref{sec:FlashOpti:Scans} and~\ref{sec:FlashOpti:Joins} will investigate when the query plan changes between disk and Flash, and what performance is lost when the incorrect decision is made (assuming disk but actually using Flash).

\section{Durable and Recoverable Transactions}
\label{sec:Background:Recovery}

Many database applications require transaction semantics commonly described as ACID (Atomic, Consistent, Isolated, Durable) \cite{Gray81}.
While the first three are primarily impacted by the database's concurrency control mechanisms within main memory, transaction durability and database recovery interacts with persistent storage.
The goal of recovery management is to ensure that during normal transaction processing no transaction reports that it has committed and is later lost after a system failure, and that any transaction that fails to commit before a failure is completely removed (i.e., no partial updates remain).
Additionally, recovery should occur as quickly as possible and allow efficient forward processing.
Several schemes provide correct, high performance recovery for disk.
I describe ARIES \cite{MohanHaderle92}, a popular Write Ahead Logging (WAL) system that provides atomic durable transactions.

\textbf{ARIES.}
ARIES uses a two-level store (disk and volatile buffer cache) alongside a centralized log.
The buffer cache is necessary to accelerate reads and defer writes to the disk.
Transaction writes coalesce in the buffer cache while being durably recorded in the log as ordered entries that describe page updates and actions, transforming random writes into sequential writes.

The log improves disk write performance while simultaneously providing data recovery after failure.
Transaction updates produce both redo and undo entries.
Redo logs record actions performed on heap pages so that they can be replayed if data has not yet written to disk in-place.
Undo logs provide roll back operations necessary to remove aborted and uncommitted transaction updates during recovery.
The database occasionally places a checkpoint in the log, marking the oldest update within the database still volatile in the buffer cache (and therefore where recovery must begin).
Recovery replays the redo log from the most recent checkpoint to the log's end, reproducing the state at failure in the buffer cache; ARIES ``replays history," recovering the failed database.
Afterwards, incomplete transactions are removed using the appropriate undo log entries.

While a centralized log orders all database updates, the software additionally enforces that log entries are durable before heap pages write back for each operation.
Transactions commit by generating a commit log entry, which must necessarily become durable after the transaction's other log entries (since the log writes to disk in order).
This process guarantees that no transaction commits, or page writes back to disk, without a durable history of its modifications in the log.

Though complex, ARIES improves database performance with disk.
First, log writes appear as sequential accesses to disk, maximizing device throughput.
Additionally, aside from reads resulting from buffer cache misses, each transaction depends on device access only at commit to flush log entries.
All disk writes to the heap may be done at a later time, off of transactions' critical paths.
In this way ARIES is designed from the ground up to minimize the effect of large disk access latencies.

\section{Memory consistency models}
\label{sec:Background:MemoryConsistency}

This section provides a background on memory consistency models, outlining three models: Sequential Consistency (SC), Total Store Order (TSO), Relaxed Memory Order (RMO), and Release Consistency (RC).
Much of this discussion assumes that caches are completely coherent---that is, any two accesses to a cache line (by any core/thread) have a total order.
Release Consistency may guarantee this property only for properly annotated programs.
Other systems enforce cache coherence in hardware without annotation.

Consistency models define the order of loads and stores observed by threads.
While every thread observes its own execution in program order, it may appear that other threads execute out of order.
Processors (and compilers) are generally free to reorder instructions to accelerate performance so long as they produce equivalent results assuming no shared memory accesses (single thread execution).
Loads and stores that are independent from a single-threaded point of view may in fact interact with other threads.
Reordering these memory accesses often results in unintended program behavior.

Two popular solutions to this problem are to (1) force all threads to observe the loads and stores of other threads in a single globally defined order (SC) or (2) relax this guarantee, introducing memory barriers that allow the programmer to enforce a certain order when necessary (i.e., relax consistency).
While relaxing consistency may provide higher performance, it places a greater burden on programmers to be aware of instruction reordering and correctly use memory barriers.

The consistency model provides the programming abstraction and guarantees on observed memory orders that the programmer can expect.
Implementations may momentarily violate the consistency model so long as no program is able to observe the violation.
For example, implementations are free to \emph{speculate}, executing with relaxed consistency, and later determine whether consistency has been violated.
When consistency is violated the implementation must rollback and re-execute memory instructions, providing the illusion that threads execute with strict consistency.

\textbf{Sequential Consistency.}
Sequential Consistency \cite{Lamport79} provides the most intuitive programming model, yet necessarily the worst performance (although modern techniques use speculation to improve performance).
All loads and stores appear in a globally consistent order that is an interleaving of program order of all threads.
A popular view of SC is that memory switches between the various processors; only a single processor accesses memory at any single time.
Additionally, many architectures define atomic Read-Modify-Write operations (RMW; e.g., compare-and-swap, atomic add).
The programmer need not consider memory instructions reordering; barriers are unnecessary (compiler barriers must still be used).

\textbf{Total Store Order.}
Total Store Order \cite{SPARCv9} provides greater performance than SC at the cost of requiring the programmer to insert memory barriers.
Most memory operations in each thread are still observed to occur in program order: (1) stores may not reorder with other stores, (2) loads may not reorder with other loads, and (3) a store that occurs after a load may not reorder and appear to occur before that load.
These rules guarantee that all stores occur in a globally consistent sequential order.
However, loads that occur after a store in program order may reorder and bypass the store, appearing to occur before the store from the perspective of other processors.
The justification for doing this is that stores are typically not on the application's critical path -- they simply write into the cache and do not result in delays.
Loads, on the other hand, may stall due to cache misses.
Executing loads as soon as possible minimizes delays.

The programmer is responsible for recognizing any code where allowing a load to bypass a store causes an incorrect result.
A barrier is provided by the architecture for such cases to force all loads to delay until the store appears to other threads (or re-execute those loads if some other thread issues conflicting stores).
RMW instructions act as as barrier, preventing memory operations from reordering.
As concurrent programming commonly uses RMW operations, explicit memory barriers are rarely required in practice.

\textbf{Relaxed Memory Order.}
Relaxed Memory Order \cite{SPARCv9} further relaxes consistency, allowing stores and loads to reorder around each other.
To enforce the intended behavior the programmer must use memory barriers to constrain memory operation order.
RMO barriers constrain (1) loads from reordering with other loads, (2) stores from reordering with other stores, (3) loads from bypassing stores, and (4) stores from bypassing loads.
Any of these constraints may be enforced at a memory barrier.
RMO requires frequent barriers for correct thread communication.

\textbf{Release Consistency.}
Release Consistency \cite{GharachorlooLenoski90} provides a relaxed consistency model with two barriers -- acquire and release.
In the absence of barriers threads may observe memory operations in any order and may read stale data (some other processor wrote to an address but an older value is loaded).
Before reading shared memory an \emph{acquire} barrier must be used to label the access as an acquire event.
Similarly, after writing to shared memory a \emph{release} barrier is used; acquire and release used as a pair ensure that written data is observed by other processors.
Additionally, acquire and release barriers prevent memory operations from reordering (for example, to ensure that operations in a critical section occur only after the lock is acquired and before the lock is released).
RMW operations implicitly contain both acquire and release barriers.

Release Consistency improves performance by (1) relaxing all consistency guarantees for single threaded code, (2) providing precise memory barriers, and (3) enforcing cache coherence only at barriers.
Acquire and release barriers order only certain types of accesses, leaving other accesses unconstrained.
However, release consistency requires the programmer to be aware of and understand these persist barriers.

A similar trade-off exists for enforcing data persistence---Persistence constraints are relaxed so that persist order may not match program order and may not appear as a valid interleaving of all persists.
Removing NVRAM write constraints improves performance while still providing mechanisms to enforce expected behavior.
The interaction between persistence, performance, and programmability is explored in Chapter~\ref{chap:Persistency}.

\section{Conclusion}
\label{sec:Background:Conclusion}

This chapter provides a brief history and outline of storage devices, database technologies, and memory consistency interfaces.
The remainder of thesis builds on these ideas to explore how best to use NVRAM in future memory systems.
