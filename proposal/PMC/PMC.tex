This chapter motivates and defines \emph{Persistent Memory Consistency}.
The ideas here constitute in-progress or future work and I welcome feedback.
Refer to Section~\ref{Background:ConsistencyModels} for a discussion of existing memory consistency models.
I first define the need for persistenct consistency and then give examples of persistent memory consistency models.
In the next chapter I outline persistent programming patterns, why they require relaxed persistent consistency models, and possible optimizations.

\section{Introduction}
\label{sec:PMC:Intro}

Future NVRAMs will provide a persistent store with the programming interface of modern main memories.
While such technologies could revolutionize the design of recoverable systems and durable storage, questions remain regarding device performance and the NVRAM programming model.
Chapters~\ref{chap:OLTP_design} and~\ref{chap:OLTP_eval} considered an abstract \emph{persist barrier} capable of enforcing persist order or blocking until previous persists complete.
Instead of considering an implementation or exact semantics of these barriers, I instead looked at the effect average persist barrier latency had on OLTP software design.
This chapter delves into the details of persist barriers, considering their implementation, performance impact, and programming model.

Persist barriers exist as a tool for programmers to ensure correct persistence behavior, while at the same time improving performance.
An alternative programming model requires that persists occur immediately -- execution does not continue until data persists successfully to NVRAM.
Such a model guarantees on recovery that the persistent state resembles a valid interleaving of program orders from the various threads.
That is, we observe some persistent state of sequentially consistent execution.
Whereas Sequential Consistency is a popular model for DRAM programming, long latency NVRAM persists may incur substantial stalls on each persist.
Persist barriers remove these stalls by allowing persists between barriers to occur in parallel.

Persist order may additionally be enforced by flushing select cache lines or flushing the entire cache, either blocking until all data persists or receiving an acknowledgement/interrupt.
This model most closely matches existing disk interfaces, where the programmer invokes system calls to write individual pages and then calls a sync system call.
However, enforcing persistence via cache line flushing requires the programmer to reason about the cache architecture and data layout (e.g., considering if an object crosses cache line boundaries).
Instead, we would like a more natural approach to reasoning about persistence that fits existing programming models.

Additional questions remain when memory is shared between threads or processes.
Currently, memory consistency models define how threads communicate and what barriers are necessary to ensure expected behavior.
Memory consistency models exist because processors (and compilers) prefer to run instructions out of order -- a pervasive optimization that significantly complicates multi-threaded programming.
While consistency models control the order in which threads observe reads and writes, there is no similar definition for the order in which persists occur, or how persist order is determined across communicating threads.
Furthermore, persistent memories often care about the actual timing of persists, not just relative ordering -- for example, system calls must make sure that all previous persists have completed before communicating to the outside world.
In this case there is no alternative but to block until all data persists.
These mechanisms are not present in existing consistency models.

While memory consistency models provide a starting point for persistent consistency models, the performance differences of volatile memory systems/DRAM and NVRAM requires new programming models for NVRAM.
In fact, the consistency and persistence models may be de-coupled.
That is, the rules that define load and store order might be differ between how \emph{values} are communicated and how \emph{persist order} is enforced.

I wish to extend memory consistency models with persistence semantics to address these concerns.
My goals are to determine how multi-threaded consistency interacts with persistence, how to relax persistent consistency models to provide high performance and easily programmable interfaces, and identify programming patterns likely to cause NVRAM performance bottlenecks alongside potential optimizations.
While a comprehensive investigation into persistent memory consistency involves considering numerous consistency models, several implementations, and broad range of data structures, I wish to restrict this dissertation to motivating a need for persistent consistency models.
To that end, I propose several relaxed consistency models.
I will implement simple, significant persistent data structures with each of these models, and plan on developing an evaluation framework for these data structures and consistency models.
Importantly, I \emph{do not wish to consider hardware implementations} of these models.
While the performance of persistent systems will invariably depend on the implementation, I feel that a basic understanding of these consistency models is a substantial contribution in itself.
My primary challenge is to devise an implementation-agnostic evaluation of these models and I welcome any feedback.

The remainder of this chapter discusses the performance implications of persist dependencies before considering examples of persistent memory consistency models.
In addition, I examine previous work, highlighting strengths and weaknesses, and placing it into my persistent memory consistency taxonomy.

\section{Implication of Persist Dependencies}
\label{sec:PMC:Performance}

Here I describe how I expect NVRAM, the memory system, and the programming model to affect performance.
Later sections will use these assumptions to reason about consistency model and data structure performance.

There are primarily two cases where persists may cause a thread to stall: ordering barriers and sync barriers.
Ordering barriers may operate by stalling execution at the barrier until all previous persists complete.
More complex memory systems will allow threads to continue executing ahead of persistent state, buffering persists (in the cache or elsewhere in the memory system), and later persisting in the proper order.
While decoupling volatile execution state and persistent state removes immediate stalls, these buffers may fill, forcing threads to stall until room becomes available.
Therefore, persist throughput is the primary concern -- buffers allow threads to execute ahead of persistent state, but the average rate of persist must match the average rate of volatile execution, otherwise stalls necessarily occur.

Persist throughput is primarily limited by the number of persist dependencies in an application.
NVRAM memories will allow high throughput so long as persists may occur in parallel.
Persist dependencies reduce parallelism, requiring that some set of writes persist entirely before any dependent persists begin (necessary for proper recovery).
Since NVRAM cells may take up to microseconds to persist, the longest dependence chain of persists limits persist throughput, additionally limiting execution throughput when buffers fill.
Persistent memory consistency models stand to improve throughput and reduce stalls by relaxing persist ordering constraints, minimizing the persist critical path to only the persist order dependencies necessary for recovery correctness.
The goal is to reduce persist critical path sufficiently that persist throughput exceeds execution throughput, providing the throughput of a DRAM system with the persistence of NVRAM.

Persistent memory applications also introduce stalls at sync barriers.
These are barriers used when interacting with the outside world (e.g., displaying something on-screen, communicating over the network).
Unless this external communication can be asynchronously ordered with persists, allowing the thread to continue doing other useful work, the thread must stall for previous persists to complete, reducing throughput.
Additionally, end-to-end latency is important for many applications and tasks -- greater sync time (while persists drain/flush) leads to increased task latency.
Again, persist critical path determines sync latency; if independent writes persist in parallel the time to sync is determined by the longest chain of persists outstanding at the time sync is called.

Finally, a persistent memory consistency model may reduce performance if barriers intended to enforce persist order additionally enforce constraints on volatile execution (e.g., a persist barriers prevents memory instructions to volatile addresses from reordering).
While I do not discuss this phenomenon further, I would like to investigate its effects in the future and would appreciate feedback.

The remainder of this chapter proposes persistent memory consistency models to relax persist constraints and improve NVRAM performance.

\section{Persistent Memory Consistency Models}
\label{sec:PMC:PersistenceModels}

While memory consistency models restrict the order that loads and stores are observed across threads, they give no guarantees on persistence.
I outline several persistent consistency models, starting with a constrained, strict model, and then considering more relaxed models.
The purpose of these models is to provide correctness by defining the allowable persistent states during execution.
Any implementation may only allow these persistent states.
Implementations are free to insert further restrictions, disallowing additional persistent states.
Additionally, implementations may \fixme{talk about speculation}
Specific implementations are outside the scope of this proposal.
I welcome any feedback.

\subsection{Persistent Sequential Consistency}
\label{sec:PMC:PersistenceModels:PSC}

The first model couples persistence to the sequential consistency model as Persistent Sequential Consistency (PSC).
This model requires that the underlying volatile memory consistency model is sequential consistency.
All loads and stores (including persists) appear to occur in a globally defined order as an interleaving of threads' valid program orders.
Whereas volatile memory systems provide this solely by controlling the order in which memory actions become visible from each processor, NVRAM used for recovery must treat every point in time as a possible failure, observing the persistent state.
Achieving a globally consistent persist order requires that persists \emph{actually} occur in-order from each thread, or that sequential batches of persists occur atomically, so that failure may not observe an intermediate state (NVRAM's atomically persistable size is likely to be small -- 8 bytes usually assumed).
Additionally, all data sharing propagates ordering dependencies between persists.

{
\singlespacing
\newsavebox{\PSCThreadOne}
\begin{lrbox}{\PSCThreadOne}
  \begin{lstlisting}
Thread 1:

A = 1
C = B
  \end{lstlisting}
\end{lrbox}

\newsavebox{\PSCThreadTwo}
\begin{lrbox}{\PSCThreadTwo}
  \begin{lstlisting}
Thread 2:

B = 1
D = A
  \end{lstlisting}
\end{lrbox}

\begin{figure}[]
\centering
\subfigure{ \usebox{\PSCThreadOne} }
\hspace{1 in}
\subfigure{ \usebox{\PSCThreadTwo} }
\caption{\textbf{Persistent Sequential Consistency (PSC).} All variables in NVRAM and initialized to 0.  PSC prevents states \{A=0, D=1\} and \{B=0, C=1\}.}
\label{fig:PSC}
\end{figure}
}

Consider Figure~\ref{fig:PSC}.
The letters correspond to persistent variables, and all are initialized to 0.
Assuming threads execute under a sequential consistency model (i.e., they observe shared values from that model) and persist orders are enforced according to the PSC model, at no point may we observe that B=0 and C=1.
Doing so would violate the model by allowing thread 1 to observe a persist from thread 2 (B=1) and then persist an additional value before B persists.
Even if the program executes according to SC, persistence order must also be observed.

A strict implementation of PSC requires that all persists occur before each thread makes any further progress -- volatile execution is coupled to persistent execution.
This strict model does not require a sync barrier.
A slight relaxation allows volatile state to advance ahead of persistent state, so long as (1) both the volatile and nonvolatile state are valid sequential executions, and (2) the persistent state matches some previous volatile state.
This optimization does require a sync barrier; communicating with the outside world requires persistent state catch up with volatile state.

The problem with PSC is that all persists within a thread are ordered, and all shared memory accesses additionally order persists.
This is also what makes PSC the most intuitive programming model.
When persist latency is large (expected to be at least hundreds of nanoseconds up to several microseconds) every persist incurs this penalty; there is no opportunity to execute persists from the same thread in parallel.
The next model relaxes the multi-threaded persist-order constraints.

\subsection{Local Persist Order}
\label{sec:PMC:PersistenceModels:LPO}

This model allows thread communication and data persistence to occur at different granularities.
Imagine, as an example, the \GroupCommit design from Section~\ref{sec:OLTP_design:GroupCommit}.
In this design I required a volatile staging buffer, separate from the primary NVRAM store, where batch updates occur.
Only at the end of the batch would I copy the original batch data to an undo log and persist batch updates in-place to NVRAM.
An alternative design omits this staging buffer by allowing updates during a batch to persist directly to the NVRAM address space.
However, each update must first check if the address region being modified is protected by a segment of the undo log, and if it is not first create and persist the necessary undo.
Once a region of memory is protected by undo, threads may update the NVRAM store in-place for the remainder of the batch without considering the order in which data persists.
At the end of the batch the batch manager enforces that the last version of data persists before invalidating the log and allowing transactions to commit.

Consider this design under PSC -- all persists within a thread occur in-order, and whenever data is shared a persist order is enforced.
However, there is no need to enforce a persist order across threads except with respect to individual memory addresses.
Data to different addresses may persist in any order so long as 1) associated undo log persists first and 2) the batch manager receives proper acknowledgement at the end of the batch that latest data versions have successfully persisted.
Additional cross-thread dependences introduce unnecessary persist-order constraints that increase the critical path of persists, limiting persist throughput.

I relax cross-thread dependences in a model called \emph{Local Persist Order} (LPO).
All persists within a thread continue to execute in-order, as in sequential consistency.
However, each thread may progress persistent state to various degrees, violating persistent sequential consistency; memory sharing does not enforce a persist-order dependence.
While persists from threads to different addresses may reorder freely, persists to the same address must serialize.
LPO relies on the underlying memory consistency model to provide this serial order (and therefore the memory consistency model/coherence implementation must provide a serial persist order on individual addresses).
For example, again consider Figure~\ref{fig:PSC}.
It is now possible to observe A=1, C=1, but B=0.
This occurs if volatile execution observes thread 2 executing entirely before thread 1, but thread 1 persists entirely before thread 2 (which is allowed according to LPO).

The previous situation may be avoided by using explicit persist barriers, which are necessary to enforce persist order between threads.
There are many types of barriers, with varying implementations.
I highlight a few here.
A thread may enforce that all local persists occur before persists from other threads that read the local values.
Such \emph{persist order-before} barriers are useful for communicating persistence to another thread.
A thread may enforce that all remote persists occur before subsequent local persists in a \emph{persist order-after}, useful for reading persists from remote threads.
Both of these may be combined in a \emph{persist total-order}, allowing a thread to completely order all persistent updates with other threads.
While these define persistent state, execution still relies on an underlying consistency model to determine what values are communicated and which threads interact.

The batching example might enforce batch persistence by having all transaction, once done persisting data, emit a \emph{persist order-before} and atomically increment a counter/semaphore (initialized to 0).
The batch manager polls this counter, and once it reaches the total number of threads (signifying that all threads are done persisting), it emits a \emph{persist order-after} barrier -- all persists from the batch manager must occur after transaction thread persists.
The batch manager invalidates the log, committing the batch.
LPO's guarantee that persists to individual addresses maintain proper order ensure that the last version of data from the batch persist.
The use of persist barriers ensures that all store data persist before the batch commits, but during batch execution extraneous cross-thread persist dependences do not slow execution.
By using a relaxed persistent memory consistency model we are able to design a more intuitive batching system (no longer requiring a volatile staging buffer) and improve persist performance.
Additionally, data persist while the batch executes, minimizing inter-batch delays.

\input{PMC/Fig.LPO.tex}
LPO potentially improves performance by minimizing the critical path of persist dependencies, shown in Figure~\ref{figure:LPO}.
Data sharing and the resulting cross-thread persist dependencies increase the critical path of persists.
The Figure displays the persist dependencies of two threads as well as shared dependencies.
Strict models, such as PSC, order persists between threads, increasing persist critical path (red arrow).
LPO, on the other hand, removes such dependencies, except where explicitly created by persist barriers.
Since threads persist independently, adding threads provides additional persist throughput, similarly to Symmetric Multi Threading (SMT), which provides memory parallelism on existing systems.
LPO, then, is an appropriate model where threads persist independently (no sharing occurs) or where memory sharing should not enforce persist order (such as batching).
However, the need to perform all persists in serial within a thread remains a concern.
The Byte Addressable File System (BPFS) addresses intra-thread persist dependencies by introducing additional persist barriers.

\subsection{Byte Addressable Persistent File System}
\label{sec:PMC:PersistenceModels:BPFS}

The Byte Addressable Persistent File System (BPFS) \cite{ConditNightingale09} is the de-facto standard for existing persistent memory consistency models, used additionally by \cite{CoburnCaulfield11, FangHsiao11, VenkataramanTolia11}.
Threads persist by writing to the persistent address space, enforcing persist order via \emph{epoch barriers} which divide persists into \emph{epochs}.
Persists may not reorder across epoch barriers, but persists within an epoch are free to execute in parallel.
BPFS additionally provides a hardware design, enforcing persist order with only cache modifications (little additional buffering or modifications to the memory system).
A table of epochs is kept in the cache of each CPU, with each cache line keeping track of its persist epoch number and process/thread ID.
The cache writes back in the background, making sure that each epoch persists completely before starting to flush the next epoch.

Data sharing and persist order is enforced strictly by stalling.
If at any point a thread overwrites a cache line from a previous persist epoch (even from the same thread), it stalls until the previous persist epoch completes persisting.
Further, if a thread reads a cache line from a persist epoch produced by a remote thread that has not yet persisted it also stalls until that epoch completes persisting (it does not stall if the reading thread is the same as the producing thread).
Persists within epochs write to the device in parallel, volatile execution proceeds ahead of persistent state, and cross-thread persist order is correctly enforced (at the cost of stalls).

\input{PMC/Fig.persist_wal.tex}

Consider the ARIES log from Chapter~\ref{chap:OLTP_design}, demonstrated in \texttt{persist\_wal} (Figure~\ref{fig::Code}) with persist dependencies shown in Figure~\ref{fig:persist_wal_persist}.
Persisting a single entry requires first persisting the entry's body, followed by the tail LSN.
Most log entries are large (at least 100 bytes), containing information about the transaction, store, page, tuple, and action taken.
According to PSC and LPO all persists (8 byte segments) to the log entry body must occur in order -- an unnecessary constraint that increases persist critical path.
Instead, BPFS allows the entire log entry body to persist in parallel, and guarantees that the body persists before the LSN using a persist barrier.

In consistency terms BPFS attempts to produce a total ordering of epochs that all threads observe (Total Epoch Order -- TEO -- would be a great name for this model).
This ordering resembles transactional memory -- producing a total order on memory transactions -- except BPFS does not provide concurrency control (conflicts are not detected and transactions can not roll back).
However, attempting to provide a total epoch order produces problems; multi-threaded BPFS execution may result in deadlock and restricts useful programming patterns.
I will demonstrate that the implementation as described allows deadlock.

{
\singlespacing
\newsavebox{\ThreadOne}
\begin{lrbox}{\ThreadOne}
  \begin{lstlisting}
Thread 1:

persist_bar
A = 1
B = 1
persist_bar
  \end{lstlisting}
\end{lrbox}

\newsavebox{\ThreadTwo}
\begin{lrbox}{\ThreadTwo}
  \begin{lstlisting}
Thread 2:

persist_bar
B = 2
A = 2
persist_bar
  \end{lstlisting}
\end{lrbox}

\begin{figure}[]
\centering
\subfigure{ \usebox{\ThreadOne} }
\hspace{1 in}
\subfigure{ \usebox{\ThreadTwo} }
\caption{\textbf{BPFS deadlock.} BPFS attempts to produce a total order on epochs.  Races within epochs create a cyclic order-dependence, resulting in deadlock.}
\label{fig:BPFS_deadlock}
\end{figure}
}

Consider Figure~\ref{fig:BPFS_deadlock}.
Each thread runs a single epoch with two persists to shared memory.
The second thread executes these persists in opposite order of the first.
If thread 1 writes to A at the same time thread 2 writes to B a deadlock occurs.
Thread 1 next attempts to write to B, fetching the cache line for B, and observes that thread 2 has not yet persisted, so thread 1's epoch must be ordered after thread 2's epoch.
Similarly, thread 2 attempts to write to A, ordering its epoch after thread 1's; a cycle has occurred, preventing forward progress.

This problem may occur even without data races or when persists have a total order.
Imagine that A and B on each thread are different variables, each protected by locks, but that thread 1's A (B) is on the same \emph{cache line} as thread 2's A (B).
Such \emph{false sharing} results in performance problems for existing volatile cache systems, but in the case of NVRAM and BPFS produces deadlock.
Furthermore, one might try to solve this problem by reordering thread 2's writes so that both thread 1 and thread 2 write in the same order, totally ordering the epochs (whoever accesses A first runs to completion while the other stalls).
However, relaxed consistency models allow the processor (or compiler) to reorder writes and persists within an epoch, arriving at the original problem.
In fact, it is not clear if instructions accessing only volatile data are permitted (or should be permitted) to reorder across epoch boundaries.
Next, A and B may represent synchronization races; consider atomic read-modify-write (such as compare-and-swap) to persistent address spaces.
BPFS, in assuming no races to persistent data, restricts the use of persistent locks.
Finally, certain data structures may specifically intend to allow races within persist epochs.
The \GroupCommit implementation described above, for example, requires threads to work concurrently towards a persistent state.

BPFS attempts to produce a total order of all epochs, producing deadlocks and preventing useful programming patterns.
I believe that the solution is to better define the interaction between persistence and the consistency model.
I next introduce a model strongly influenced by BPFS, that allows concurrent epochs to make forward progress at the cost of a slightly less intuitive interface.

\subsection{Partial Epoch Order}
\label{sec:PMC:PersistenceModels:PEO}

The third and final model relaxes persist order within a thread similarly to BPFS, but produces only a partial order of epochs between threads.
Partial Epoch Order (PEO) considers the epochs of two threads that communicate through memory to be \emph{concurrent}, and thus not ordered.
However, later epochs \emph{do} carry the dependence, allowing ordering of persists between threads while avoiding deadlock and allowing inter-thread persist concurrency.
As in LPO persists from concurrent epochs to the same address must have a total order, provided by the underlying consistency model.

{
\singlespacing
\newsavebox{\PEOThreadOne}
\begin{lrbox}{\PEOThreadOne}
  \begin{lstlisting}
Thread 1:

persist_bar (i)
persist A

persist_bar (ii)
persist X
write C

persist_bar (iii)
persist D
  \end{lstlisting}
\end{lrbox}

\newsavebox{\PEOThreadTwo}
\begin{lrbox}{\PEOThreadTwo}
  \begin{lstlisting}
Thread 1:

persist_bar (i)
persist B

persist_bar (ii)
read C
persist Y

persist_bar (iii)
persist E
  \end{lstlisting}
\end{lrbox}

\begin{figure}[]
\centering
\subfigure{ \usebox{\PEOThreadOne} }
\hspace{1 in}
\subfigure{ \usebox{\PEOThreadTwo} }
\caption{\textbf{Partial Epoch Order (PEO).} PEO considers epochs in different threads containing a memory dependence to be concurrent.  This memory dependence does not enforce a persist order between the threads' epochs, but enforces this order on earlier and later epochs.  E must persist after A.}
\label{fig:PEO}
\end{figure}
}

Figure~\ref{fig:PEO} provides an example.
Two threads persist data and communicate, each with three epochs.
Communication occurs in epoch (ii) of each thread -- thread 1 writes to (volatile) address C, which thread 2 reads.
According to PEO, epochs (ii) of each thread are concurrent and do not enforce a persist order.
Thus, thread 1's persist to X may not necessarily occur before thread 2's persist to Y.
However, epochs of thread 1 prior to (ii) must necessarily persist before epochs of thread 2 after (ii).
Thread 2's persist to A must persist before thread 2's persist to E -- epochs containing shared memory accesses are concurrent, but later epochs carry the persist dependence.
The converse is not true -- thread 2's persist to B need not persist before thread 1's persist to D.
This model produces a partial ordering of persist epochs that all threads observe.

PEO removes the possibility of deadlock by relaxing persist order constraints.
However, PEO supports the same programming model as BPFS (a total epoch order).
Any program who's epochs contain only volatile races, or only persists (never both) will produce a total order on epochs containing persists, resulting in the same execution as BPFS.
Additionally, PEO's relaxed constraints allow advanced programming models such as batching.
Neglecting undo log (which must persist before store data) transactions in a batch may all execute within the same persist epoch.
Since these epochs are concurrent no persist order is enforced between persists of different threads.
When the batch ends transactions end their epoch and increment the semaphore/counter, as in Section~\ref{sec:PMC:PersistenceModels:LPO}.
The batch manager polls the counter until it matches the number of transaction threads, ends its own epoch, and invalidates the log.
In practice, transaction threads must use multiple epochs to order undo log persists before data persists; I would like to evaluate whether PEO provides sufficiently relaxed persist constraints or if other models (such as LPO) are required for batching performance.

It is unclear to me if volatile memory and non-memory instructions should be allowed to reorder with persist epoch barriers.
This will depend on the actual implementation, and if the hardware is able to easily distinguish persists from nonvolatile writes.
For example, thread 2's read to C in the previous example should be allowed to occur before the persist to B so long as the correct epoch order is enforced.
As persist barriers might be frequent, not allowing non-persistent instructions to reorder would limit the performance of \emph{volatile} memory instructions.

PEO provides effective single-thread persist parallelism while providing a relatively intuitive model for cross-thread persist-order dependencies.
PEO prevents deadlock and allows concurrent access to shared persistent memory spaces without enforcing a persist order.

\section{Conclusion}
\label{sec:PMC:Conclusion}

This chapter motivated the need for more precise persistent consistency models to allow greater persist performance and enforce correctness while providing an easy to program interface.
I proposed three new persistent consistency models and relate them to existing work.
While this chapter has focused on persistence semantics and the programming interface, the next chapter demonstrates common persistent data structures, focusing on potential performance concerns.
